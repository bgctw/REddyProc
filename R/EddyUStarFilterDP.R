#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+++ R script with sEddyProc methods for ustar filtering +++
#+++ Ustar filtering adapted after the idea in Papale, D. et al. (2006) +++
#+++ Dependencies: Eddy.R
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

sEddyProc$methods(
	sEstUstarThreshold = structure(function(
	  ##title<<
	  ## sEddyProc$sEstUstarThreshold - Estimating ustar threshold
	  ##description<<
	  ## Estimate the Ustar threshold by aggregating the estimates for seasonal and temperature subsets.
	  ##author<<
	  ## OM, TW
		ds = sDATA					##<< data.frame with columns named by the following arguments
		,UstarColName = "Ustar"		##<< collumn name for UStar
		,NEEColName = "NEE"			##<< collumn name for NEE
		,TempColName = "Tair"		##<< collumn name for air temperature
		,RgColName = "Rg"			##<< collumn name for solar radiation for omitting night time data
		,seasonFactor.v = createSeasonFactorMonth(ds$sDateTime) ##<< factor for subsetting times see details
		,seasonFactorsYear = getYearOfSeason(seasonFactor.v, ds$sDateTime)   ##<< named integer vector: for each seasonFactor level, get the year that this season belongs to  
		,ctrlUstarEst.l = controlUstarEst()			##<< control parameters for estimating uStar on a single binned series, see \code{\link{controlUstarEst}}
		,ctrlUstarSub.l = controlUstarSubsetting()	##<< control parameters for subsetting time series (number of temperature and Ustar classes \ldots), see \code{\link{controlUstarSubsetting}} 
		,fEstimateUStarBinned = estUstarThresholdSingleFw2Binned	##<< function to estimate UStar on a single binned series, see \code{\link{estUstarThresholdSingleFw2Binned}}
		,isCleaned=FALSE			##<< set to TRUE to avoid call to \code{\link{cleanUStarSeries}}.
){
	##references<< 
  ## Ustar filtering following the idea in Papale, D. et al. (2006)  
  ## Towards a standardized processing of net ecosystem exchange measured with eddy covariance technique: algorithms and uncertainty estimation.
  ## Biogeosciences 3(4): 571-583.
    
	# subset for first season
	# TODO: implement seasonal functionality
	#dsi <- subset(ds, seasonFactor.v == 1)
	#					
	##details<<
	## The threshold for sufficiently turbulent conditions u* (Ustar) 
	## is estimated for different subsets of the time series.
	## From the estimates for each season (each value in \code{seasonFactor.v}) 
	## the maximum is reported as global estimate.
	## Within each season the time series is split by temperature classes. 
	## Among these Ustar estimates, the median is reported as season value.
	##
	## In order to split the seasons, the uses has provide a vector with argument \code{seasonFactor.v}
	## where rows with the same value belong to
	## the same season. It is conveniently generated by one of the following functions:
	## \itemize{
	## 	\item{ \code{\link{createSeasonFactorMonthWithinYear}} (default DJF-MAM-JJA-SON) }
	## 	\item{ \code{\link{createSeasonFactorYday}} }
	## } 
	##
	## The estimation of Ustar on a single binned series can be selected argument \code{fEstimateUStarBinned}.
	## \itemize{
	## 	\item{ \code{\link{estUstarThresholdSingleFw1Binned}} }
	## 	\item{ \code{\link{estUstarThresholdSingleFw2Binned}} (default) }
	## } 
	##
	## This function is usually called by
	## \itemize{
	## \item{ \code{\link{sEstUstarThresholdYears}} that applies this function to subsets of each year. }
	## \item{ \code{\link{sEstUstarThresholdDistribution}} which additionally estimates median and confidence intervals for each year by bootstrapping the original data.}
	## } 
	#
	dsF <- cbind(ds,season=seasonFactor.v)
	dsc <- if( isCleaned) dsF else cleanUStarSeries( dsF, UstarColName, NEEColName, TempColName, RgColName, ctrlUstarSub.l$swThr)
	#
	tdsc <- as.data.frame(table(dsc$season)); colnames(tdsc) <- c("season","nRec")
	#some seasons might be absent in dsc from cleaning, construct vectors that report NA for missing seasons
	nRecValidInSeason <- merge( data.frame( season=sort(unique(dsF$season)) ), tdsc, all.x=TRUE)
	nRecValidInSeasonYear <- merge(nRecValidInSeason, data.frame(season=names(seasonFactorsYear), year=seasonFactorsYear), all.x=TRUE)
	nYear <- ddply(nRecValidInSeasonYear, .(year), summarize, nRec=sum(nRec, na.rm=TRUE) )
	##details<< \describe{\item{One-big-season fallback}{
	## If there are too few records within one year, there is  fallback to assemble the data from 
	## the different seasons to one big season.
	## By default, a warning is issued. The user can suppress the fallback by provinding option
	## \code{ctrlUstarSub.l$isUsingOneBigSeasonOnFewRecords = FALSE} (see \code{\link{controlUstarSubsetting}})
	## }}
	if( nrow(dsc)==0L ) stop("sEstUstarThreshold: no finite records in dataset")
	yearsWithFewData <- nYear$year[ nYear$nRec < ctrlUstarSub.l$minRecordsWithinYear ]
	nRecValidInSeasonYear$seasonAgg <- nRecValidInSeasonYear$season
	# year <- yearsWithFewData[1] 
	for( year in yearsWithFewData){
		if( ctrlUstarSub.l$isUsingOneBigSeasonOnFewRecords == FALSE ){
			stop("sEstUstarThreshold: too few finite records within one year ", year 
					," (n=",nYear$nRec[ nYear$year==year ]
					,"). Need at least n=",ctrlUstarSub.l$minRecordsWithinYear
					," Or set ctrlUstarSub.l$isUsingOneBigSeasonOnFewRecords to TRUE."
			)
		} else {
			warning("sEstUstarThreshold: too few finite records within one year ",year,". Aggregating all data of this year to one big season.")
			seasons <- nRecValidInSeasonYear$season[nRecValidInSeasonYear$year == year]
			dsc$season[ dsc$season %in% seasons ] <- seasons[1]
			nRecValidInSeasonYear$seasonAgg[ nRecValidInSeasonYear$season %in% seasons] <- seasons[1]
		}
	}
	#
	#dsi <- subset(dsc, season == 4)
	#dsi <- subset(dsc, season == 0)
	if( isTRUE(ctrlUstarEst.l$isUsingCPTSeveralT)){
		##details<< 
		## When using changePoint detection (CPT) method, unreasonable or non-significant breakpoints are already
		## excluded by F-test to linear model.
		## Hence, here the median over temperature classes is taken instead of the median.
		## In addition, with CPT seasons are excluded where a threshold was detected in only less 
		## than ctrlUstarEst.l$minValidUStarTempClassesProp (default 20% ) of the 
		## temperature classes
		UstarSeasonsTemp <- daply(dsc, .(season), .estimateUStarSeasonCPTSeveralT, .drop_o = FALSE, .inform = TRUE
				#, .drop_o = FALSE, .inform = TRUE
				,ctrlUstarSub.l = ctrlUstarSub.l
				,ctrlUstarEst.l = ctrlUstarEst.l
				,fEstimateUStarBinned = fEstimateUStarBinned
				,UstarColName = UstarColName
				,NEEColName = NEEColName
				,TempColName = TempColName
				,RgColName = RgColName
		) # daply over seasons  matrix (nSeason x nTemp)
		uStarSeasons <- apply( UstarSeasonsTemp, 1, median, na.rm=TRUE)
		iNonValid <- rowSums(is.finite(UstarSeasonsTemp))/ncol(UstarSeasonsTemp) < ctrlUstarEst.l$minValidUStarTempClassesProp
		uStarSeasons[iNonValid] <- NA_real_
		# need check to avoid -Inf in max function
	} else {
		UstarSeasonsTemp <- daply(dsc, .(season), .estimateUStarSeason, .drop_o = FALSE, .inform = TRUE
				,ctrlUstarSub.l = ctrlUstarSub.l
				,ctrlUstarEst.l = ctrlUstarEst.l
				,fEstimateUStarBinned = fEstimateUStarBinned
				,UstarColName = UstarColName
				,NEEColName = NEEColName
				,TempColName = TempColName
				,RgColName = RgColName
		) # daply over seasons  matrix (nTemp x nSeason)
		uStarSeasons <- apply( UstarSeasonsTemp, 1, median, na.rm=TRUE)
		# different to C-version, report NA where threshold was found in less than 20% of temperature classes
		iNonValid <- rowSums(is.finite(UstarSeasonsTemp))/ncol(UstarSeasonsTemp) < ctrlUstarEst.l$minValidUStarTempClassesProp
		uStarSeasons[iNonValid] <- NA_real_
	}
	results <- merge(nRecValidInSeasonYear, data.frame(seasonAgg=names(uStarSeasons),uStar=uStarSeasons), all.x=TRUE)
	uStarYear = ddply(results, .(year), function(dss){
				data.frame( uStar=if( all(!is.finite(dss$uStar))  ) NA_real_ else max( dss$uStar, na.rm=TRUE), year=dss$year[1])
			})
	uStarAggr <- median(uStarYear$uStar, na.rm=TRUE)
	message(paste("Estimated UStar threshold of: ", signif(uStarAggr,2)
					,"by using controls:\n", paste(capture.output(unlist(ctrlUstarSub.l)),collapse="\n")
			))
	resultsDf <- results[,c("season","year","uStar")]
	resultsDf$aggregationMode <- "season"
	resultsDf <- tmp <- rbind(cbind(data.frame(aggregationMode="year", season=as.factor(NA_integer_)), uStarYear),resultsDf )
	resultsDf <- tmp <- rbind(cbind(data.frame(aggregationMode="single", season=as.factor(NA), year=NA_integer_), uStar=uStarAggr),resultsDf )
	##value<< A list with entries
	list(
			uStarTh = resultsDf[,c("aggregationMode","year","season","uStar")]	##<< data.frame with columns "aggregationMode","year","season","uStar" 
				## with rows for "single": the entire aggregate (median across years)
				##, "year": each year (maximum across seasons)
				##, "season": each season (median across temperature classes)
			,seasonAggregation =  nRecValidInSeasonYear	##<< data.frame listing for each season, the number of valid records, and the seasons it was aggregated to in case of few records 
			,UstarSeasonTemp=t(UstarSeasonsTemp)		##<< numeric matrix (nTemp x nAggSeason): estimates for each temperature subset for each aggregated season
	)
	## uStar values are reported for each season, together with the information on the seasons: to which aggregated season
	## it belongs (if there were too few records within year), and to which year it is associated.
},ex = function(){
  if( FALSE ) { #Do not always execute example code (e.g. on package installation)
    Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
    EddyData.F <- ds <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
    EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(EddyData.F, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
    EddyProc.C <- sEddyProc$new('DE-Tha', EddyDataWithPosix.F, c('NEE','Rg','Tair','VPD','Ustar'))   
    #ds <- head(ds,2000)
    (Result.L <- EddyProc.C$sEstUstarThreshold())
	(Results.L2 <- EddyProc.C$sEstUstarThreshold(ctrlUstarEst.l=controlUstarEst(isUsingCPTSeveralT=TRUE)))
  }
}))

.estimateUStarSeason <- function(dsi, ctrlUstarSub.l, ctrlUstarEst.l, fEstimateUStarBinned
		,UstarColName 		##<< collumn name for UStar
		,NEEColName 		##<< collumn name for NEE
		,TempColName 		##<< collumn name for air temperature
		,RgColName 			##<< collumn name for solar radiation for omitting night time data
){
	if( nrow(dsi) < ctrlUstarSub.l$minRecordsWithinSeason){
		warning("sEstUstarThreshold: too few finite records within season (n=",nrow(dsi),"). Need at least n=",ctrlUstarSub.l$minRecordsWithinSeason,". Returning NA for this Season." )
		return( rep(NA_real_, ctrlUstarSub.l$taClasses))
	}
	if( nrow(dsi)/ctrlUstarSub.l$taClasses < ctrlUstarSub.l$minRecordsWithinTemp ){
		warning("sEstUstarThreshold: too few finite records within season (n=",nrow(dsi),") for ",ctrlUstarSub.l$taClasses
				," temperature classes. Need at least n=",ctrlUstarSub.l$minRecordsWithinTemp*ctrlUstarSub.l$taClasses
				,". Returning NA for this Season." )
		return( rep(NA_real_, ctrlUstarSub.l$taClasses))
	}
	# if( as.POSIXlt(dsi$sDateTime[1])$year+1900==2002 & dsi$season[1]==2L ) recover()	
	#cat(dsi$season[1], as.POSIXlt(dsi$DateTime[1])$mon, ",")
	dsiSort <- arrange(dsi, dsi[,TempColName]) 	#sort values in a season by air temperature (later in class by ustar)
	#N <- nrow(dsi ) #number of observations (rows) total, probably can get from elsewhere..
	#T_bin_size <- round(N/ctrlUstarSub.l$taClasses) #set T_bin size so that every bin has equal #values
	#set up vector that contains Ustar values for temperature classes
	UstarTh.v = vector(length=ctrlUstarSub.l$taClasses)
	# twutz 1505: changed temperature binning of records to put equals temperatures into the same bin (compatibility with C code)
	#trace(.binWithEqualValues,recover)		#untrace(.binWithEqualValues)
	TId <- .binWithEqualValuesBalanced(dsiSort[,TempColName], ctrlUstarSub.l$taClasses)
#recover()	
	#k<-1L
	for (k in 1:ctrlUstarSub.l$taClasses){	# k temperature class
		# minimum number of records within temp checked above					
		#print(k)
		#if( k == 4 ) recover()
		#original Dario's C version...
		#ta_class_start = 0;
		#ta_class_end = season_start_index;
		#/* set start & end indexes */
		#  ta_class_start = ta_class_end;
		#ta_class_end = season_start_index + (ta_samples_count*(i+1)-1);
		
		#/om:this part only implemented for checking C code compatibility...
		#ta_class_start = ta_class_end
		#ta_class_end = ta_class_start + T_bin_size-1
		#/eom
		
		#subset into Ta classes
#		if (k==ctrlUstarSub.l$taClasses){ # use end index of vector for slightly smaller last bin (due to rounding) 
#			dsiSortTclass <- dsiSort[((k-1)*T_bin_size+1):N,]
#		} else {
#			dsiSortTclass <- dsiSort[((k-1)*T_bin_size+1):((k)*T_bin_size),]
#		}
	
		dsiSortTclass <- dsiSort[TId == k,]
		#constraint: u* threshold only accepted if T and u* are not or only weakly correlated..
		Cor1 = suppressWarnings( abs(cor(dsiSortTclass[,UstarColName],dsiSortTclass[,TempColName])) ) # maybe too few or degenerate cases
		if( inherits(Cor1,"try-error") ) recover()
		# TODO: check more correlations here? [check C code]
		#      Cor2 = abs(cor(dataMthTsort$Ustar,dataMthTsort$nee))
		#      Cor3 = abs(cor(dataMthTsort$tair,dataMthTsort$nee))
		if( (is.finite(Cor1)) && (Cor1 < ctrlUstarEst.l$corrCheck)){ #& Cor2 < CORR_CHECK & Cor3 < CORR_CHECK){
			if( isTRUE(ctrlUstarEst.l$isUsingCPT) ){
				resCPT <- try( fitSeg1(dsiSortTclass[,UstarColName], dsiSortTclass[,NEEColName]), silent=TRUE )
				UstarTh.v[k] <- if( inherits(resCPT,"try-error") || !is.finite(resCPT["p"]) || resCPT["p"] > 0.05) NA else resCPT["cp"]
			} else {
				dsiBinnedUstar <- binUstar(dsiSortTclass[,NEEColName],dsiSortTclass[,UstarColName],ctrlUstarSub.l$UstarClasses)
				#plot( NEE_avg ~ Ust_avg, dsiBinnedUstar)
				if( any(!is.finite(dsiBinnedUstar[,2])) ){
					stop("Encountered non-finite average NEE for a UStar bin.",
							"You need to provide data with non-finite collumns uStar and NEE for UStar Threshold detection.")
				}
				UstarTh.v[k] <- if( dsiBinnedUstar[1,1] > ctrlUstarEst.l$firstUStarMeanCheck ){
					##details<<
					## If the first mean uStar bin is already large (>ctrlUstarEst.l$firstUStarMeanCheck)
					## Then this temperature class is skipped from estimation
					NA_real_	
				} else {
					fEstimateUStarBinned(  dsiBinnedUstar, ctrlUstarEst.l = ctrlUstarEst.l)
				}
			}
		} else { #correlation between T and u* too high
			#fill respective cell with NA
			UstarTh.v[k] = NA
			#TODO: should a message be printed here to the user??
		}
	}
	UstarTh.v # vector of uStar for temperature classes
}


controlUstarEst <- function(
  ### Default list of parameters for determining UStar of a single binned series	
  #T Classes not needed here?
  # either?
  #,taClasses=7 # set number of ta classes   
  # or?
  #,ctrlUstarSubsetting.l = controlUstarSubsetting()
  #,taClasses=ctrlUstarSubsetting.l$taClasses  
  #
  #,percentile = 90 #percentile value... double check!
  #,percentile_check = TRUE #enable percentile check\n ... double check!
  ustPlateauFwd = 10 	##<< number of subsequent uStar bin values to compare to in fwd mode
  ,ustPlateauBack = 6	##<< number of subsequent uStar bin values to compare to in back mode  
  ,plateauCrit = 0.95	##<< significant differences between a u* value and the mean of a "plateau"
  ,corrCheck = 0.5 		##<< threshold value for correlation between Tair and u* data
  ,firstUStarMeanCheck=0.2	##<< if first uStar bin average of a class is already larger than this value, the temperature class is skipped.
  ,isOmitNoThresholdBins = TRUE	##<< if TRUE, bins where no threshold was found are ignored. Set to FALSE to report highest uStar bin for these cases
  ,isUsingCPT=FALSE		##<< set to TRUE to use changePointDetection without binning uStar before
  ,isUsingCPTSeveralT=FALSE	##<< set to TRUE to use changePointDetection without binning uStar for several temperature classifications
  ,minValidUStarTempClassesProp=0.2 ##<< seasons in only less than this proportion of temperature classes, a threshold was detected are excluded
  ,minValidBootProp=0.4	##<< minimum proportion of bootstrap samples for which a threshold was detected. Below this proportion NA quantiles are reported.
  ,minNuStarPlateau=3L  ##<< minimum number of records in plateau, threshold must be larger than mean of this many records 
  #,bt = FALSE 			##<< flag for bootstrapping
  #,btTimes = 100 		##<< number of bootstrap samples
  
  #,method.v = function... #fw2 by default..
  
  #TODO: what does the following param do?
  #define FIRST_Ustar_MEAN_CHECK  		0.2  
  # 4.) const int percentiles[PERCENTILES_COUNT] = { 5, 10, 25, 50, 75, 90, 95};
  
){
  ##seealso<< \code{\link{estUstarThresholdSingleFw2Binned}}, \code{\link{controlUstarSubsetting}} 
  ctrl <- list(  
    #taClasses=taClasses
    #,UstarClasses=UstarClasses  
    #percentile = percentile
    #percentile_check = percentile_check #enable percentile check\n ... double check!
    ustPlateauFwd = ustPlateauFwd    #number of subsequent thresholds to compare to in fwd mode
    ,ustPlateauBack = ustPlateauBack #number of subsequent thresholds to compare to in back mode  
    ,plateauCrit = plateauCrit #significant differences between a u* value and the mean of a "plateau"
    ,corrCheck = corrCheck #threshold value for correlation between Tair and u* data
	,firstUStarMeanCheck=firstUStarMeanCheck
	,isOmitNoThresholdBins = isOmitNoThresholdBins
	,isUsingCPT = isUsingCPT
	,isUsingCPTSeveralT = isUsingCPTSeveralT
	,minValidUStarTempClassesProp = minValidUStarTempClassesProp
	,minValidBootProp=minValidBootProp
	,minNuStarPlateau=minNuStarPlateau
	#,seasons = seasons # switch for three different seasonal modes 
    #(seasons or "groupby" may easily extended to an input vector or matrix)
    #,bt = bt #flag for bootstrapping
    #,btTimes = btTimes #number of bootstrap samples
  )
  #display warning message for the following variables that we advise not to be changed
  if (corrCheck != 0.5) warning("WARNING: parameter corrCheck set to non default value!")
  ctrl
}
attr(controlUstarEst,"ex") <- function(){
	controlUstarEst()
}

controlUstarSubsetting <- function(
	### Default list of parameters for determining UStar of a single binned series	
	taClasses=7 		##<< set number of air temperature classes 
	,UstarClasses=20 	##<< set number of Ustar classes 	
	# seasons param deprecated
  # TODO: add seasons handling to documentation
  #,seasons = 1 # switch for different seasonal modes #TODO: Update?!
	,swThr = 10  		##<< nighttime data threshold for solar radion [Wm-2]
	,minRecordsWithinTemp = 100		##<< integer scalar: the minimum number of Records within one Temperature-class
	,minRecordsWithinSeason = 160	##<< integer scalar: the minimum number of Records within one season
	,minRecordsWithinYear	= 3000	##<< integer scalar: the minimum number of Records within one year
	,isUsingOneBigSeasonOnFewRecords = TRUE ##<< boolean scalar: set to FALSE to avoid aggregating all seasons on too few records
	# 1.) ,selection parameter for which fwd and back modes? fwd2 as default... 
	# 2.) ,MIN_VALUE_PERIOD <<- 3000 # per whole data set... double check C code
	# 3.) ,MIN_VALUE_SEASON <<- 160 #if #number of data points in one any season are smaller than that, merge to one big season
	#define MIN_VALUE_PERIOD    		3000		/* min values for compute u* threshold */
	#define MIN_VALUE_SEASON				160			/* min for seasons */
	#define TA_CLASS_MIN_SAMPLE				100
  ){  
	##seealso<< \code{\link{estUstarThresholdSingleFw2Binned}}, \code{\link{controlUstarSubsetting}} 
	ctrl <- list(
    	taClasses=taClasses
		,UstarClasses= UstarClasses
  		#,seasons
  		,swThr = swThr
		,minRecordsWithinTemp = minRecordsWithinTemp
		,minRecordsWithinSeason = minRecordsWithinSeason
		,minRecordsWithinYear = minRecordsWithinYear
		,isUsingOneBigSeasonOnFewRecords = isUsingOneBigSeasonOnFewRecords
)	
  if (ctrl$swThr != 10) warning("WARNING: parameter swThr set to non default value!")
  if (ctrl$taClasses != 7) warning("WARNING: parameter taClasses set to non default value!")	
  if (ctrl$UstarClasses != 20) warning("WARNING: parameter UstarClasses set to non default value!")
  if (ctrl$minRecordsWithinTemp != 100) warning("WARNING: parameter minRecordsWithinTemp set to non default value!")
  if (ctrl$minRecordsWithinSeason != 160) warning("WARNING: parameter minRecordsWithinSeason set to non default value!")
  if (ctrl$minRecordsWithinYear != 3000) warning("WARNING: parameter minRecordsWithinYear set to non default value!")
  ctrl
}
attr(controlUstarSubsetting,"ex") <- function(){
	controlUstarSubsetting()
}

createSeasonFactorMonthWithinYear <- function(
	### calculate factors to denote the season for uStar-Filtering by specifying starting months, with seasons not spanning year boundaries
  	dates							##<< POSIXct vector of length of the data set to be filled				
  	, month=as.POSIXlt(dates)$mon   ##<< integer (0-11) vector of length of the data set to be filled, specifying the month for each record
	, year=as.POSIXlt(dates)$year+1900	##<< integer vector of length of the data set to be filled, specifying the year 
	, startMonth=c(3,6,9,12)-1		##<< integer vector specifying the starting month for each season, counting from zero, default is (Dez,Jan,Feb)(Mar,April,May)(June,July,August),(Sept,Okt,Nov)
){
  ##seealso<< \code{\link{createSeasonFactorYday}}
  ##details<< 
  ## If Jan is not a starting month, then the first months of each year will be 
  ## part of the last period in the year.
  ## E.g. with the default the fourth period of the first year consists of Jan,Feb,Dec.
  if( length(year) == 1L) year <- rep(year, length(month))
  if( length(month) != length(year) ) stop("Month and Year arguments need to have the same length.")
  startMonth <- sort(unique(startMonth))
  boLastPeriod <- month < startMonth[1] 		
  # translate month before the first specified beginning month to be after last specified month (1 becomes 13)
  month[ boLastPeriod ] <- month[ boLastPeriod] +12
  startMonthAdd <- c(startMonth, startMonth[1]+12)
  seasonFac <- year*1000L + rep(0L, length(month) )
  # i <- 2
  for( i in 2:length(startMonth) ){
	  bo <- month >= startMonthAdd[i] & month < startMonthAdd[i+1]
	  seasonFac[bo] <- year[bo]*1000L + (i-1)
  }
  #plot( seasonFac ~ months )
  as.factor(seasonFac) 	
  ##value<<
  ## Integer vector length(dates), with each unique value representing one season
}
attr(createSeasonFactorMonthWithinYear,"ex") <- function(){
	Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
	EddyData.F <- dss <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
	EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(dss, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
	(res <- createSeasonFactorMonthWithinYear(ds$DateTime))
	(res2 <- createSeasonFactorYday(ds$DateTime)) # default days are chosen to correspond to start of Febr, June, Sept, and Dec
}

createSeasonFactorMonth <- function(
		### calculate factors to denote the season for uStar-Filtering by specifying starting months, with continuous seasons spanning year boundaries
		dates							##<< POSIXct vector of length of the data set to be filled				
		, month=as.POSIXlt(dates)$mon   ##<< integer (0-11) vector of length of the data set to be filled, specifying the month for each record
		, year=as.POSIXlt(dates)$year+1900	##<< integer vector of length of the data set to be filled, specifying the year 
		, startMonth=c(3,6,9,12)-1		##<< integer vector specifying the starting month for each season, counting from zero, default is (Dez,Jan,Feb)(Mar,April,May)(June,July,August),(Sept,Okt,Nov)
){
	##seealso<< \code{\link{createSeasonFactorYday}}
	##details<< 
	## If Jan is not a starting month, then the first months of each year will be 
	## part of the last period in the year.
	## E.g. with the default the fourth period of the first year consists of Jan,Feb,Dec.
	if( length(year) == 1L) year <- rep(year, length(month))
	if( length(month) != length(year) ) stop("Month and Year arguments need to have the same length.")
	starts <- data.frame(month=sort(unique(startMonth)), year=rep(sort(unique(year)),each=length(startMonth)) )
	if( starts$month[1] != 0L ) starts <- rbind( data.frame(month=0L, year=starts$year[1]),starts )
	seasonFac <- integer(length(month)) # 0L
	starts$startYearMonths <- startYearMonths <- starts$year*1000L + starts$month
	yearMonths <- year*1000L+month
	# i <- 1
	for( i in 1:(length(startYearMonths)-1) ){
		bo <- yearMonths >= startYearMonths[i] & yearMonths < startYearMonths[i+1]
		seasonFac[bo] <- starts$year[i]*1000L + starts$month[i] 
	}
	i <- length(startYearMonths)
	bo <- yearMonths >= startYearMonths[i]
	seasonFac[bo] <- starts$year[i]*1000L + starts$month[i]
	#plot( seasonFac ~ dates )
	as.factor(seasonFac) 	
	##value<<
	## Integer vector length(dates), with each unique value representing one season
}
attr(createSeasonFactorMonth,"ex") <- function(){
	Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
	EddyData.F <- dss <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
	EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(dss, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
	(res <- createSeasonFactorMonth(ds$DateTime))
	plot( res ~ ds$DateTime)
}


createSeasonFactorYday <- function(
	### calculate factors to denote the season for uStar-Filtering by specifying starting day of years
	dates							##<< POSIXct vector of length of the data set to be filled				
	, yday=as.POSIXlt(dates)$yday  ##<< integer (0-11) vector of length of the data set to be filled, specifying the month for each record
	, year=as.POSIXlt(dates)$year+1900	##<< integer vector of length of the data set to be filled, specifying the year 
	, startYday=c(335,60,152,244)-1	 ##<< integer vector (0-366) specifying the starting yearDay for each season
){
	##details<<
	## With default parameterization, dates are assumed to denote begin or center of the eddy time period.
	## If working with dates that denote the end of the period, use \code(yday=as.POSIXlt(fGetBeginOfEddyPeriod(dates))$yday
	if( length(year) == 1L) year <- rep(year, length(yday))
	if( length(yday) != length(year) ) stop("Month and Year arguments need to have the same length.")
	starts <- data.frame(yday=sort(unique(startYday)), year=rep(sort(unique(year)),each=length(startYday)) )
	if( starts$yday[1] != 0L ) starts <- rbind( data.frame(yday=0L, year=starts$year[1]),starts )
	seasonFac <- integer(length(yday)) # 0L
	starts$startYearDays <- startYearDays <- starts$year*1000L + starts$yday
	yearDays <- year*1000L+yday
	# i <- 1
	for( i in 1:(length(startYearDays)-1) ){
		bo <- yearDays >= startYearDays[i] & yearDays < startYearDays[i+1]
		seasonFac[bo] <- starts$year[i]*1000L + starts$yday[i] 
	}
	i <- length(startYearDays)
	bo <- yearDays >= startYearDays[i]
	seasonFac[bo] <- starts$year[i]*1000L + starts$yday[i]
	#plot( seasonFac ~ dates )
	as.factor(seasonFac) 	
	##value<<
	## Integer vector of nrow ds, each unique class representing one season
}
attr(createSeasonFactorYday,"ex") <- function(){
	Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
	EddyData.F <- dss <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
	EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(dss, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
	(res <- createSeasonFactorYday(ds$DateTime))
	plot( res ~ ds$DateTime)
}

getYearOfSeason <- function(
		## determine the year of the record of middle of seasons  
		seasonFactor.v		##<< factor vector of length data: for each record which season it belongs to 
		, sDateTime.v		##<< POSIX.t vector of length data: for each record: center of half-hour period
){
	originCt <- as.POSIXct("1970-01-01 00:00.00 UTC")
	timezone <- attr(sDateTime.v[1],"tzone")
	#dates <- sDateTime.v[seasonFactor.v == seasonFactor.v[1]]
	res <- tapply(sDateTime.v, seasonFactor.v, FUN=function(dates){
				x <- as.numeric(dates)
				xCenter <- x[1] + (x[length(x)] - x[1])/2
				1900L + as.POSIXlt(xCenter, origin=originCt, tz=timezone)$year
			})
	##value<<  named integer vector, with names corresponding to seasons
	# need to convert 1d array to vector
	structure(as.vector(res), names=rownames(res))
}
.tmp.f <- function(){
	ds <- eddyProc$sDATA
	sDateTime.v <-ds$sDateTime
	seasonFactor.v <- createSeasonFactorMonth(ds$sDateTime)
	getYearOfSeason( seasonFactor.v, sDateTime.v)
	getYearOfSeason( seasonFactor.v, sDATA$sDateTime)
}




binUstar <- function(
	### Bin the NEE for a number of classes of UStar classes
	NEE.v				##<< vector with value of Net Ecosystem exchange
	,Ustar.v 			##<< vector with u* (friction velocity (m2/s)
	,UstarClasses=controlUstarSubsetting()$UstarClasses	##<< the number of binning classes
	,isUStarSorted=FALSE	##<< set to TRUE, if NEE and Ustar are already sorted by increasin Ustar values (performance gain)

){
	ds.f <- data.frame(NEE=NEE.v,Ustar=Ustar.v)
	#within data frame sort values by Ustar
	if( !isTRUE(isUStarSorted))	ds.f <- arrange(ds.f,ds.f[,2])
	#
	# twutz 1505: changed binning to take care of equal values in uStar column 
	# when assigning uStar classes, only start a new class when uStar value changes
	ds.f$uClass <- .binWithEqualValuesMinRec(ds.f$Ustar, nBin=UstarClasses, tol = 1e-14)
	#
	ddply( ds.f, .(uClass), summarise, Ust_avg=mean(Ustar,na.rm=TRUE), NEE_avg=mean(NEE, na.rm=TRUE), nRec=length(NEE))[,-1]
}
attr(binUstar,"ex") <- function(){
	Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
	EddyData.F <- ds <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
	EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(EddyData.F, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
	dss <- subset(EddyDataWithPosix.F, DoY >= 150 & DoY <= 250)
	(res <- binUstar(dss$NEE, dss$Ustar))
	(resFW1 <- estUstarThresholdSingleFw1Binned(res))
	(resFW2 <- estUstarThresholdSingleFw2Binned(res))
}

.binWithEqualValuesBalanced <- function(
	### createg a binning factor so that equal values of x end up in the same bin, with shortening following following bins
	x				##<< sorted numeric vector to sort into bins
	,nBin			##<< intended number of bins
	,tol = 1e-8		##<< distance between successive values of x that are treated to be equal
	,isBinSizeFloorUsed=TRUE	##<< set to FALSE to postpone rounding on start and end values
){
	binSize <- length(x) / nBin
	##details<< 
	## By not taking the floor, a better distribution of samples across bins is achieved.
	## But here keep it due to compatibility to C-Code.
	if( isBinSizeFloorUsed ) binSize <- floor(binSize)
	breaksX <- which(diff(x) > tol)+1
	binId <- rep(1L,length(x))
	iBreak <- 1L	# index from which to seek next break
	#iClass <- 2L
	for( iClass in 2:nBin){
		start0 <- round((iClass-1)*binSize)+1
		iBreak <- .whichValueGreaterEqual(breaksX, start0, iStart=iBreak)
		start1 <- breaksX[iBreak]
		#start1Slow <- breaksX[breaksX >= start0][1]	# find next uStar change at or after position start0 
		binId[start1:length(x)] <- iClass
	}
	##value<< integer vector of same length as x, with unique value for each bin
	binId
}

.binWithEqualValuesMinRec <- function(
		### createg a binning factor so that equal values of x end up in the same bin, with shifting following bins
		x				##<< sorted numeric vector to sort into bins
		,nBin			##<< intended number of bins
		,tol = 1e-8		##<< distance between successive values of x that are treated to be equal
){
	lengthX <- length(x)
	binId <- integer(lengthX)
	binSize <- as.integer(floor(lengthX / nBin))
	iBreaksX <- which(diff(x) > tol)		# positions in x where value is numerically different from following element
	iBreak <- 0L		# start index in iBreaks, to avoid searching the part of samller elements 
	iEnd <- 0L			# index in x, end of the (previous) period
	iBin <- 0L			# bin Id
	while( iEnd < lengthX ){
		iBin <- iBin + 1L
		iStart <- iEnd +1L
		iEnd <- iEnd+binSize		# same as iStart + binsSize-1, with counting from 1 instead of 0 
		# find the next break after iEnd
		iBreak <- .whichValueGreaterEqual(iBreaksX, iEnd, iBreak+1L)
		if( is.na(iBreak) ){
			# no break was found, set period end to vector end and finish
			# if length of last bin is smaller than 90% of intended binsize, sort records to former bin
			if( (lengthX+1L-iStart) < binSize*0.9 && iBin != 1L)
				iBin <- iBin -1L
			binId[iStart:lengthX] <- iBin
			break
		} else {
			iEnd <- iBreaksX[iBreak]	# update iEnd to position with break after it
			binId[iStart:iEnd] <- iBin
		}
	}
	##value<< integer vector of same length as x, with unique value for each bin.
	## Each bin holds at least length(x)/nBin records, or more if there were values after the bin that were
	## numerically equal to last value of the bin.
	## The actual number of bins might be differnt from argument nBin due to numericall equal values
	## and is reported with attribute \code{nBin}
	attr(binId,"nBin") <- iBin
	binId
}

.whichValueGreaterEqual <- whichValueGreaterEqualR <- function(
	### search first element in an integer vector that is larger 
	x			##<< increasingly sorted numeric vector to search 
	, threshold	##<< integer scalar: searched element will need to be greater or equal as this argument 
	, iStart=1L	##<< index in vector to start search
){
	#which(x >= threshold)[1]
	#iStart-1 + which(x[iStart:length(x)] >= threshold)[1]
	# for performance reasons call a c++ function that loops across the vector
	whichValueGreaterEqualC( as.integer(x), as.integer(threshold), as.integer(iStart) )	# defined in cFunc.R
	##value<< 
	## Scalar integer: first index in x, that is >= iStart, and whose value x[i] is >= threshold.
	## If no index was found, returns NA
}

estUstarThresholdSingleFw1Binned <- function(
		### estimate the Ustar threshold for single subset, using FW1 algorithm, relying on binned NEE and Ustar
		Ust_bins.f			##<< data.frame with columns NEE_avg and Ust_avg, of NEE by Ustar bins by \code{\link{binUstar}}
		,ctrlUstarEst.l = controlUstarEst() ##<< parameter list, see \code{\link{controlUstarEst}} for defaults and description
){
	##references<< inspired by Papale 2006
	
	# algorithm to check when plateau is reached
	flag <- FALSE
	#for every u* bin compare to avg of subsequent UST_PLATEAU, until found
	u <- 1
	#TODO: change to for loop 1:ustClasses and then break
	# in order to avoid infinite loop in case of error
	# optimize with Thomas?
	
  while (!flag){ #only stop if threshold is found
		if (!flag & (Ust_bins.f$NEE_avg[u] >= (ctrlUstarEst.l$plateauCrit*mean(Ust_bins.f$NEE_avg[(u+1):(u+ctrlUstarEst.l$ustPlateauFwd)],na.rm=T)))){ #na.rm=T to exclude NAs out of bounds..
			#   NEE_i >= .95*avg(i,i+1,...,i+10)  [FW]    
			UstarThSingle <- Ust_bins.f$Ust_avg[u]        
			flag <- TRUE #set flag for threshold found in this mode
		}          
		#case that no threshold could be found by plateau method, use maximum u* in that T_class...
		if (u==(nrow(Ust_bins.f)-1)){ #FW1: -1 ; FW2: 
			UstarThSingle <- Ust_bins.f$Ust_avg[u+1]        
			break;      
		}
		u <- u+1 #increase index by 1
	}  
	return(UstarThSingle)
}

estUstarThresholdSingleFw2Binned <- function(
  ### estimate the Ustar threshold for single subset, using FW2 algorithm  	
  Ust_bins.f							##<< data.frame with column s NEE_avg and Ust_avg, of NEE by Ustar bins by \code{\link{binUstar}}
  ,ctrlUstarEst.l = controlUstarEst()	##<< parameter list, see \code{\link{controlUstarEst}} for defaults and description 
){  
	# algorithm to check when plateau is reached
  flag <- FALSE
  #for every u* bin compare to avg of subsequent UST_PLATEAU, until found
  u <- 1
  UstarThSingle <- NA_real_
  ##details<< 
  ## Demand that threshold is higher than \code{ctrlUstarEst.l$minNuStarPlateau} records.
  ## If fewer records  
  umax <- nrow(Ust_bins.f)-max(2L,ctrlUstarEst.l$minNuStarPlateau) # FF2 neads at least two bins after threshold
  while (u <= umax){ 
    if (
		(Ust_bins.f$NEE_avg[u] >= (ctrlUstarEst.l$plateauCrit*mean(Ust_bins.f$NEE_avg[(u+1):(u+ctrlUstarEst.l$ustPlateauFwd)],na.rm=T))) & 
		(Ust_bins.f$NEE_avg[u+1] >= (ctrlUstarEst.l$plateauCrit*mean(Ust_bins.f$NEE_avg[(u+1+1):(u+ctrlUstarEst.l$ustPlateauFwd+1)],na.rm=T)))
	){
      UstarThSingle <- Ust_bins.f$Ust_avg[u]        
	  break
    }
	u = u+1L
  }
  #case that no threshold could be found by plateau method, use maximum u* in that T_class...
  # twutz: 1505: implemented option to return NA, to omit from median over bins (C-compatibility)
  if(is.na(UstarThSingle) & !isTRUE(ctrlUstarEst.l$isOmitNoThresholdBins) ) 
	  UstarThSingle <- Ust_bins.f$Ust_avg[u+1]
recover()  
  return(UstarThSingle)    
}

.tmp.f <- function(){
	plot( Ust_bins.f$NEE_avg ~ Ust_avg, Ust_bins.f)
}

cleanUStarSeries <- function(
	### remove non-finite cases and omit night time data.
	ds						    ##<< data.frame with columns
	,UstarColName = "Ustar"  
	,NEEColName = "NEE"
	,TempColName = "Tair"
	,RgColName = "Rg"
	,swThr = controlUstarSubsetting()$swThr
){
	ds <- subset(ds, 
			is.finite(ds[,NEEColName]) & 
			is.finite(ds[,TempColName]) & 
			is.finite(ds[,UstarColName]) & 
			is.finite(ds[,RgColName]) 
	) 
	#night time data selection
	ds <- ds[ ds[,RgColName] < swThr, ]
	##value<< ds with non-finite cases and cases with radiation < swThr removed.
	ds
}

sEddyProc$methods(
		sEstUstarThresholdDistribution = structure(function(
		### Estimating the distribution of u* threshold by bootstrapping over data
		#ds					    ##<< data.frame with columns see \code{\link{sEstUstarThresholdYears}}
		ctrlUstarEst.l = controlUstarEst()			##<< control parameters for estimating uStar on a single binned series, see \code{\link{controlUstarEst}}
		,ctrlUstarSub.l = controlUstarSubsetting()	##<< control parameters for subsetting time series (number of temperature and Ustar classes \ldots), see \code{\link{controlUstarSubsetting}} 
		,...						##<< further arguments to \code{\link{sEstUstarThresholdYears}}
		,seasonFactor.v = createSeasonFactorMonth(sDATA$sDateTime)   ##<< factor of seasons to split (need to permute the same way as sData)
		,seasonFactorsYear = getYearOfSeason(seasonFactor.v, ds$sDateTime)   ##<< named integer vector: for each seasonFactor level, get the year that this season belongs to  
		,nSample = 100L				##<< the number of repetitions in the bootstrap
        ,probs = c(0.05,0.5,0.95)	##<< the quantiles of the bootstrap sample to return. Default is the 5%, median and 95% of the bootstrap
){
		##details<< 
		## The choice of the criterion for sufficiently turbulent conditions (u* > choosen threshold)
		## introduces large uncertainties in calculations based on gap-filled Eddy data.
	  ## Hence, it is good practice to compare derived quantities based on gap-filled data using different u* threshold values.
		  
		##
		## This method explores the probability density of the threshold by repeating its estimation
		## on a bootstrapped sample.
		## By default it returns the 90% confidence interval (arguement \code{probs}). 
		## For larger intervals the sample number need to be increased (arguement \code{probs}). 
		
		##seealso<< \code{\link{sEstUstarThreshold}}, \code{\link{sEstUstarThresholdYears}}, \code{\link{sMDSwithUStarDP}}
		ds <- sDATA
		ds$seasonFactor.v <- seasonFactor.v
		res0 <- suppressMessages(.self$sEstUstarThreshold(ds,
				...
				,ctrlUstarEst.l =ctrlUstarEst.l, ctrlUstarSub.l=ctrlUstarSub.l
				, seasonFactor.v=seasonFactor.v	))
		iPosAgg <- which(res0$uStarTh$aggregationMode=="single")
		iPosYears <- which(res0$uStarTh$aggregationMode=="year")
		iPosSeasons <- which(res0$uStarTh$aggregationMode=="season")
		years0 <- res0$uStarTh$year[iPosYears]
		seasons0 <- res0$uStarTh$season[iPosSeasons]
		fWrapper <- function(iSample, ...){
			dsBootWithinSeason <- ds2 <- ddply(ds, .(seasonFactor.v), function(dss) {
						iSample <- sample.int(nrow(dss),replace=TRUE)
						dss[iSample, ,drop=FALSE]
					})
			cat(".")
			res <- .self$sEstUstarThreshold(dsBootWithinSeason, ...
							, seasonFactor.v=seasonFactor.v, seasonFactorsYear=seasonFactorsYear
							, ctrlUstarEst.l =ctrlUstarEst.l, ctrlUstarSub.l=ctrlUstarSub.l	)
			gc()
			# need to check if years and seasons have been calculated differently due to subsetting with
			# too few values within a season
			# then report NA for those cases
			resAgg <- res$uStarTh$uStar[iPosAgg] 
			years <- res$uStarTh$year[iPosYears]
			resYears <- structure( 
					if( all(years == years0) ) res$uStarTh$uStar[iPosYears] else rep(NA_real_, length(years0))
					, names=as.character(years0) )
			resSeasons <- structure( 
					if( nrow(res$uStarTh) == nrow(res0$uStarTh) && all((seasons <- res$uStarTh$season[iPosSeasons]) == seasons0) ) res$uStarTh$uStar[iPosSeasons] else rep(NA_real_, length(seasons0))
					, names=as.character(seasons0) )
			return(c(aggYears=resAgg, resYears, resSeasons ))
			#return(length(res$UstarSeason$uStar))
			#res$UstarAggr
		}
		Ustar.l0 <- res0$uStarTh$uStar[c(iPosAgg, iPosYears, iPosSeasons)]	
		Ustar.l <- suppressMessages(
				Ustar.l <- lapply(1:(nSample-1), fWrapper,...)
		)
		cat("\n")
		stat <- do.call( rbind, c(list(Ustar.l0),Ustar.l))
		##details<<
		## If more than \code{ctrlUstarEst.l$minValidBootProp} did not report a treshold, 
		## quantiles are set to NA.
		resQuantiles <-	t(apply( stat, 2, quantile, probs=probs, na.rm=TRUE ))
		iInvalid <- colSums(is.finite(stat))/nrow(stat) < ctrlUstarEst.l$minValidBootProp 
		resQuantiles[iInvalid,] <- NA_real_
		resDf <- cbind(res0$uStarTh, resQuantiles)
		message(paste("Estimated UStar distribution of:\n", paste(capture.output(resDf[resDf$aggregationMode=="single",-(1:3)]),collapse="\n")
						,"\nby using ",nSample,"bootstrap samples and controls:\n", paste(capture.output(unlist(ctrlUstarSub.l)),collapse="\n")
				))
		resDf
		##value<< 
		## A data.frame with columsn aggregationMode, year, and original UStar estimate 
		## The other columns correpond to the quantiles of Ustar estimate
		## for given probabilities (argument \code{probs} ).
},ex = function(){
	if( FALSE ){	# takes long, so do not execute on each install or check
		# load the data and generate DateTime column
		Dir.s <- paste(system.file(package='REddyProc'), 'examples', sep='/')
		EddyData.F <- ds <- fLoadTXTIntoDataframe('Example_DETha98.txt', Dir.s)
		EddyDataWithPosix.F <- ds <- fConvertTimeToPosix(EddyData.F, 'YDH', Year.s='Year', Day.s='DoY', Hour.s='Hour')
		EddyProc.C <- sEddyProc$new('DE-Tha', EddyDataWithPosix.F, c('NEE','Rg','Tair','VPD','Ustar'))
		# initialize parallel setup and do the bootstrap, may omit
		#sfInit(parallel=TRUE,cpus=2)
		#options("boot.parallel" = "snow")
		#getOption("boot.parallel")
		#options("boot.parallel" = NULL)
		(res <- EddyProc.C$sEstUstarThresholdDistribution(nSample=10))
		getAnnualSeasonUStarMappingFromDistributionResult(res)
	}
}) )

getAnnualSeasonUStarMappingFromDistributionResult <- function(
		### extract mapping season -> uStar columns from Distribution result (\code{\link{sEstUstarThresholdDistribution}})
		uStarTh
){
	dsYear <- subset(uStarTh,aggregationMode=="year")
	dsYear$season <- NULL
	dsYear$aggregationMode <- NULL
	##details<<
	## rows with no threshold get the aggregated value, i.e. the median of columns of other years
	naLines <- which(apply(dsYear[,-(1) ,drop=FALSE],1,function(x){ all(is.na(x))}))
	if( length(naLines) ){
		medianYear <- apply( dsYear[!naLines,-(1),drop=FALSE], 2, median, na.rm=TRUE )
		dsYear[naLines,-(1)] <- medianYear
	}
	dsSeasons <- subset(uStarTh,aggregationMode=="season",select=c("season","year"))
	res2 <- merge(dsSeasons, dsYear )
	res2$year <- NULL
	# transform column names of "x%" to "Ux" with leading zeros
	colnames(res2)[-(1:2)] <- (gsub(" ","0",sprintf("U%2s",gsub("%","",colnames(res2)[-(1:2)]))))
	res2
}

getSeaonalSeasonUStarMappingFromDistributionResult <- function(
		### extract mapping season -> uStar columns from Distribution result (\code{\link{sEstUstarThresholdDistribution}})
		uStarTh
){
	dsSeasons <- subset(uStarTh,aggregationMode=="season")[-(1:2)]
	##details<<
	## missing thresholds are replace by corresponding estimates based on annually aggregated estimates
	## (\code{\link{getAnnualSeasonUStarMappingFromDistributionResult}})
	naLines <- apply(dsSeasons[,-(1),drop=FALSE],1,function(x){ all(is.na(x))})
	if( length(naLines) ){
		dsYears <- getAnnualSeasonUStarMappingFromDistributionResult(uStarTh)
		dsSeasons[naLines,] <- dsYears[naLines,]
	}
	##value<< a data frame with first column the season, and other columns different uStar threshold estimates
	# transform column names of "x%" to "Ux" with leading zeros
	colnames(dsSeasons)[-(1:2)] <- (gsub(" ","0",sprintf("U%2s",gsub("%","",colnames(dsSeasons)[-(1:2)]))))
	dsSeasons
}



